{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector & Graph Retrieval Augmented Generation\n",
    "\n",
    "In this notebook we will be using the LLama Index toolkit to store and retrieve our dataset that will be used as input into our LLM Use case. The goal here is to demonstrate how we can enhance a standard LLM to answer use case specific questions with high accuracy\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pypdf pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the llama_\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index import KnowledgeGraphIndex\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "import pickle\n",
    "\n",
    "from pyvis.network import Network\n",
    "\n",
    "from ipywidgets import GridspecLayout\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Key Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"insert_OPENAI_API_KEY_here\"\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\",api_base='https://jpmcproxy-j2v7yxjplq-nw.a.run.app/v1', api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents we are querying are related to BABA, AliBaba , recent articles from Seeking Alpha as well as earnings call transcript. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_folder = './documents/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First a simple example implementing RAG via Vector Embeddings:\n",
    "\n",
    "This code allows us to query the documents, retrieve relevant content from the documents with respect to the question, then this will be sent to the LLM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the directory into LLamas directory reader\n",
    "documents = SimpleDirectoryReader(document_folder).load_data()\n",
    "\n",
    "#Index the documents \n",
    "simple_rag_index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "#Set your query engine\n",
    "rag_query_engine = simple_rag_index.as_query_engine()\n",
    "\n",
    "\n",
    "#Pass your question against the vector store to receive the relevant context and pass to the LLM\n",
    "response = rag_query_engine.query(\"How are Alibaba doing?\")\n",
    "\n",
    "print(f\"Here is the model output: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example implementing RAG via Knowledge Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size_limit=512)\n",
    "\n",
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "kg_index = KnowledgeGraphIndex.from_documents(documents=documents,storage_context=storage_context,service_context=service_context)\n",
    "kg_query_engine = kg_index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the knowledge graph index as a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kg_index.pkl', 'wb') as f:\n",
    "    pickle.dump(kg_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use graphing tool called networkx https://networkx.org/documentation/stable/tutorial.html that supports graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = kg_index.get_networkx_graph()\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "net.show(\"AlibabaGraph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking the same question using the knowledge graph to collect relevant information with respects to the question being asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = kg_query_engine.query('What are the most divergent opinions on Alibaba?')\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(prompt):\n",
    "    return rag_query_engine.query(prompt)\n",
    "\n",
    "def kg_query(prompt):\n",
    "    return kg_query_engine.query(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple UI that allows you to ask questions with respects to the documents using both techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridspecLayout(8,6)\n",
    "\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description='Query',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Press to submit',\n",
    "    icon='lock',\n",
    "    layout=widgets.Layout(height='auto', width='auto'),\n",
    ")\n",
    "\n",
    "input_box = widgets.Textarea(\n",
    "    value=None,\n",
    "    placeholder='Type a question about your documents',\n",
    "    description='Question:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(height='auto', width='auto'),\n",
    "    rows=4\n",
    ")\n",
    "\n",
    "results_box = widgets.Textarea(\n",
    "    value=None,\n",
    "    placeholder='Results...',\n",
    "    description='Answer:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(height='auto', width='auto'),\n",
    "    rows=4\n",
    ")\n",
    "\n",
    "# we will populate this later with our list of methods.\n",
    "dropdown = widgets.Select(\n",
    "    options=['RAG','KG Rag'],\n",
    "    value='RAG',\n",
    "    # rows=10,\n",
    "    description='Method:',\n",
    "    disabled=False\n",
    ")\n",
    "filter_methods = {'RAG':rag_query,'KG Rag':kg_query}\n",
    "dropdown.options = (filter_methods.keys())\n",
    "\n",
    "grid[4:7,5] = dropdown\n",
    "grid[1:4,:5] = input_box\n",
    "grid[1:3,5] = submit_button\n",
    "grid[4:8,:5] = results_box\n",
    "\n",
    "# anywhere you can now just update the variable and it will live update.\n",
    "def question(e):\n",
    "    user_input = input_box.value\n",
    "    method = filter_methods[dropdown.value]\n",
    "    results_box.value=method(user_input).response\n",
    "\n",
    "submit_button.on_click(question)\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
